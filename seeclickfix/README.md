# SeeClickFix

Workflow for fetching Improve Detroit issues from the SeeClickFix API, publishing them as a Socrata dataset, and doing analysis.

## Context

The Improve Detroit app is built on the SeeClickFix platform, which makes data about issues available through two APIs: a public endpoint and an organization endpoint that requires authentication. The organization enpoint gives us more fields for the same set of issues.

We publish issues from the public endpoint on our Socrata Open Data Portal, found [here](https://data.detroitmi.gov/Government/Improve-Detroit-Submitted-Issues/fwz3-w3yn).

Different city departments are interested in using the issue data to track various metrics on a monthly or weekly basis.

## SeeClickFix -> Socrata

How we get data from the APIs into Socrata.
 
### Public endpoint

We follow the process outlined in [Socrata's Connectors & ETL Templates repository](https://socrata.github.io/connectors/templates/see-click-fix-to-socrata.html). This workflow relies on a python script, FME Workbench (a visual editor for data integration), and the Socrata Writer.

### Organization endpoint

We wrote our own python script to flatten nested data fields, paginate through the issues, and write them to a .csv that we can upload as a Socrata dataset. This one doesn't use FME Workbench.

Run it (using python 3.6): `python scf_to_csv.py`

## Analysis

Some stats we're interested in tracking:
- How many issues of each type are opened?
- How many days pass from when an issue is created to when it's closed? Is this time within the Service Level Agreements different departments commit to?
- What types of issues are most likely to be re-opened? To be duplicates?

Two ways to approach the analysis:
1. Within a custom date range, illustrated in `02_analyze_range.ipynb`
2. Over a series of time (like every day, week, month, etc), as in `02_analyze_series.ipynb`

Before running either of these, first run `01_transform.ipynb` to clean-up and enrich the issue data. The .csv imported there is generated by running `scf_to_csv.py`.

Each of the analysis files has a `config` variable for defining your own parameters.

## Next steps

- `scf_to_csv.py` currently grabs all issues, which we want to create our initial dataset. But from then on, we want to scrape just the new or updated issues every day and add them to the existing dataset. We're still figuring out the best way to capture and fetch only what's changed
- document how we configure DataSync and Windows Task Manager to automatically run this workflow once a day
- we should also adjust pandas to read/query the initial data as json directly from Socrata instead of importing a .csv
- keep building out the time series analysis
- add test coverage
